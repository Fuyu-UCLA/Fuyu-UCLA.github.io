---
layout: post
title:  "Homework 4"
categories: blog assignment
---

```python
import pandas as pd
import tensorflow as tf
```

<h1>§1. Acquire Training Data</h1>

Each row of this data set corresponds to an article with 3 columns including:
* title
* full article text
* information that can be used to determine if an article is false or true (True if 0, False if 1)

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"

train_data = pd.read_csv(train_url)
```

```python
print(len(train_data))
train_data.head()
```

![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic1.png) 


<h1>§2. Make a Dataset</h1>

"make_dataset" function do two things:
1. Remove stopwords from the article text and title. A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” 
2. Construct and return a <b>tf.data.Dataset</b> with two inputs and one output. 

Then batch the Dataset prior to returning it for the model to train on chunks of data rather than individual rows.


```python
def make_dataset(dataset):
    # make a stopwords list with sklearn.feature_extraction.text.ENGLISH_STOP_WORDS
    from sklearn.feature_extraction import text
    stop = text.ENGLISH_STOP_WORDS
    
    # remove stopwords from title and text in dataset
    dataset['text'] = dataset['text'].apply(lambda x: " ".join([word  for word in x.split() if word not in stop]))
    dataset['title'] = dataset['title'].apply(lambda x: " ".join([word for word in x.split() if word not in stop]))

    # make dataset including input with title and text columns and output with fake column dataset
    my_data_set = tf.data.Dataset.from_tensor_slices(({"title": dataset['title'], "text": dataset['text']}, {"fake": dataset['fake']}))

    # batch the dataset 
    return my_data_set.batch(100)
```

Call the function defined above

```python
my_data_set = make_dataset(train_data)
```

Make validation dataset, train dataset, and test dataset.

Their sizes is: 20% for validation. Of the remaining 80%, 80% is train data and the rest is test data.

```python
val_size = int(len(my_data_set) * 0.2)
train_size = int((len(my_data_set) - val_size)*0.8)

val_data_set = my_data_set.take(val_size)
train_data_set = my_data_set.skip(val_size).take(train_size)
test_data_set = my_data_set.skip(val_size + train_size)

print(len(val_data_set), len(train_data_set), len(test_data_set))
```
output: 45 144 36

<h2>TextVectorization</h2>

<!-- ここにんText vectorization の説明を入れる！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！ -->

```python
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
import re
import string

#preparing a text vectorization layer for tf model
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

title_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

title_vectorize_layer.adapt(train_data_set.map(lambda x, y: x["title"]))
```

```python
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

title_input = keras.Input(
    shape=(1, ),
    name="title", 
    dtype="string")

text_input = keras.Input(
    shape=(1, ),
    name="text",
    dtype="string")
```

``python
title_features = title_vectorize_layer(title_input)
title_features = layers.Embedding(size_vocabulary, output_dim = 3, name="embedding1")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)

text_features = layers.Dense(32, activation='relu')(text_input)
text_features = title_vectorize_layer(text_input)
text_features = layers.Embedding(size_vocabulary, output_dim = 3, name="embedding2")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
```
```python
num_fakes = len(train_data["fake"].unique())
num_fakes
```

```python
main1 = layers.concatenate([title_features], axis = 1)
main1 = layers.Dense(32, activation='relu')(main1)
output1 = layers.Dense(num_fakes, name="fake")(main1) 

main2 = layers.concatenate([text_features], axis = 1)
main2 = layers.Dense(32, activation='relu')(main2)
output2 = layers.Dense(num_fakes, name="fake")(main2) 

main3 = layers.concatenate([title_features, text_features], axis = 1)
main3 = layers.Dense(32, activation='relu')(main3)
output3 = layers.Dense(num_fakes, name="fake")(main3) 
```

```python
model1 = keras.Model(
    inputs = [title_input],
    outputs = output1
)

model2 = keras.Model(
    inputs = [text_input],
    outputs = output2
)

model3 = keras.Model(
    inputs = [title_input, text_input],
    outputs = output3
)
```

```python
from tensorflow.keras import utils
utils.plot_model(model1)
```
![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic2.png) 
``python
utils.plot_model(model2)
```
![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic3.png) 
```python
utils.plot_model(model3)
```
![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic4.png) 
```python
model1.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

model2.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

model3.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
```

```python
history = model1.fit(train_data_set, 
                    validation_data=val_data_set,
                    epochs = 50, 
                    verbose = False)
```

```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
```
![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic5.png) 
```python
history2 = model2.fit(train_data_set, 
                    validation_data=val_data_set,
                    epochs = 50, 
                    verbose = False)
```

```python
plt.plot(history2.history["accuracy"])
plt.plot(history2.history["val_accuracy"])
```
![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic6.png) 

```python
history3 = model3.fit(train_data_set, 
                    validation_data=val_data_set,
                    epochs = 50, 
                    verbose = False)
```

```python
plt.plot(history3.history["accuracy"])
plt.plot(history3.history["val_accuracy"])
```
![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic7.png) 

```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_data = pd.read_csv(test_url)
test_data.head()
```
![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic8.png) 

```python
test_data_set = make_dataset(test_data)
```

```python
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split()
```