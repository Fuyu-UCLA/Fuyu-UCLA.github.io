---
layout: post
title:  "Machine Learning2 - Fake News Classification"
categories: blog assignment
---

```python
import pandas as pd
import tensorflow as tf
```

<h1>§1. Acquire Training Data</h1>

Each row of this data set corresponds to an article with 3 columns including:
* title
* full article text
* information that can be used to determine if an article is false or true (True if 0, False if 1)

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"

train_data = pd.read_csv(train_url)
```

```python
print(len(train_data))
train_data.head()
```

![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic1.png){: height="200px" width="auto"}


<h1>§2. Make a Dataset</h1>

"make_dataset" function do two things:
1. Remove stopwords from the article text and title. A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” 
2. Construct and return a <b>tf.data.Dataset</b> with two inputs and one output. 

Then batch the Dataset prior to returning it for the model to train on chunks of data rather than individual rows.


```python
def make_dataset(dataset):
    # make a stopwords list with sklearn.feature_extraction.text.ENGLISH_STOP_WORDS
    from sklearn.feature_extraction import text
    stop = text.ENGLISH_STOP_WORDS
    
    # remove stopwords from title and text in dataset
    dataset['text'] = dataset['text'].apply(lambda x: " ".join([word  for word in x.split() if word not in stop]))
    dataset['title'] = dataset['title'].apply(lambda x: " ".join([word for word in x.split() if word not in stop]))

    # make dataset including input with title and text columns and output with fake column dataset
    my_data_set = tf.data.Dataset.from_tensor_slices(({"title": dataset['title'], "text": dataset['text']}, {"fake": dataset['fake']}))

    # batch the dataset 
    return my_data_set.batch(100)
```

Call the function defined above

```python
my_data_set = make_dataset(train_data)
```

Make validation dataset, train dataset, and test dataset.

Their sizes is: 20% for validation, 10% for test, 70% for train.

```python
val_size = int(len(my_data_set) * 0.2)
train_size = int(len(my_data_set)*0.7)

val_data_set = my_data_set.take(val_size)
train_data_set = my_data_set.skip(val_size).take(train_size)
test_data_set = my_data_set.skip(val_size + train_size)

print(len(val_data_set), len(train_data_set), len(test_data_set))
```
output: 45 157 23

<h2>TextVectorization</h2>

Define Text Vectorization and make a text vectorize layer for "title" of dataset.

```python
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
import re
import string

#preparing a text vectorization layer for tf model
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

title_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

title_vectorize_layer.adapt(train_data_set.map(lambda x, y: x["title"]))
```
Make a text vectorize layer for "text" of dataset.

```python
text_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

text_vectorize_layer.adapt(train_data_set.map(lambda x, y: x["text"]))
```

<h1>§3. Create Models</h1>

<h2>When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?</h2>

To address this question, create three (3) TensorFlow models.
1. In the first model, use only the article title as an input.
2. In the second model, use only the article text as an input.
3. In the third model, use both the article title and the article text as input.

First, import some livraries I use and make Input object for "title" and "text".

```python
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

title_input = keras.Input(
    shape=(1, ),
    name="title", 
    dtype="string")

text_input = keras.Input(
    shape=(1, ),
    name="text",
    dtype="string")
```

Using the Functional API and implement layers.

First put in the vectorization layer, then rebuild the string word into a vector representation with Embedding. Dropout prevents over-fitting and compresses the information received from Embedding with GrobalAveragePooling1D. Then, Dropout is performed again, and finally Dense is performed.

```python
title_features = title_vectorize_layer(title_input)
title_features = layers.Embedding(size_vocabulary, output_dim = 3, name="embedding1")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(1, activation='relu')(title_features)

text_features = layers.Dense(32, activation='relu')(text_input)
text_features = text_vectorize_layer(text_input)
text_features = layers.Embedding(size_vocabulary, output_dim = 3, name="embedding2")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
```

Get number of fake class.

```python
num_fakes = len(train_data["fake"].unique())
num_fakes
```

Prepare for 3 models explained above.

```python
main1 = layers.concatenate([title_features], axis = 1)
main1 = layers.Dense(32, activation='relu')(main1)
output1 = layers.Dense(num_fakes, name="fake")(main1) 

main2 = layers.concatenate([text_features], axis = 1)
main2 = layers.Dense(32, activation='relu')(main2)
output2 = layers.Dense(num_fakes, name="fake")(main2) 

main3 = layers.concatenate([title_features, text_features], axis = 1)
main3 = layers.Dense(32, activation='relu')(main3)
output3 = layers.Dense(num_fakes, name="fake")(main3) 
```

Create 3 models: 
1. use only the article title as an input.
2. use only the article text as an input.
3. use both the article title and the article text as input.

```python
model1 = keras.Model(
    inputs = [title_input],
    outputs = output1
)

model2 = keras.Model(
    inputs = [text_input],
    outputs = output2
)

model3 = keras.Model(
    inputs = [title_input, text_input],
    outputs = output3
)
```
<h2>visualize your models with this code:</h2>

```python
from tensorflow.keras import utils
utils.plot_model(model1)
```
![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic2-2.png){: height="500px" width="auto"}

Over-fitting has occurred and the data is not helpful at all.

```python
utils.plot_model(model2)
```
![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic3-2.png){: height="500px" width="auto"}


```python
utils.plot_model(model3)
```
![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic4-2.png){: height="500px" width="auto"}

Compile models

```python
model1.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

model2.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

model3.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
```

Training models, 

```python
history = model1.fit(train_data_set, 
                    validation_data=val_data_set,
                    epochs = 50, 
                    verbose = False)
```

Visualize the resulting graphs

```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
```
![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic5-2.png)

Over-fitting has occurred and the data is not helpful at all.

```python
history2 = model2.fit(train_data_set, 
                    validation_data=val_data_set,
                    epochs = 50, 
                    verbose = False)
```

```python
plt.plot(history2.history["accuracy"])
plt.plot(history2.history["val_accuracy"])
```
![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic6-2.png) 

Looks really good.

```python
history3 = model3.fit(train_data_set, 
                    validation_data=val_data_set,
                    epochs = 50, 
                    verbose = False)
```

```python
plt.plot(history3.history["accuracy"])
plt.plot(history3.history["val_accuracy"])
```
![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic7-2.png) 

It is worse than the result of model2

<h2>Therefore, model 2; only using text data, to classificate.</h2>


<h1>§4. Model Evaluation</h1>


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_data = pd.read_csv(test_url)
test_data.head()
```
![image-example.png]({{ site.baseurl }}/images/pic16B-HW4-pic8-2.png) 

```python
test_data_set = make_dataset(test_data)
```

```python
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split()
```